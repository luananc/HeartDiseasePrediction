{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.11.4\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 1.24.4 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.7.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.3.0 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 2.0.3 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m xgboost version 1.7.6 is installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m shap version 0.42.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m seaborn version 0.12.2 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.11.4\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.11\"):\n",
    "    print(FAIL, \"Python version 3.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'numpy': \"1.24.4\", 'matplotlib': \"3.7.2\",'sklearn': \"1.3.0\", \n",
    "                'pandas': \"2.0.3\",'xgboost': \"1.7.6\", 'shap': \"0.42.1\", 'seaborn': \"0.12.2\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, fbeta_score, auc,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Set\n",
      "X: (308854, 18)\n",
      "y: (308854,)\n",
      "X_subset: (3088, 18)\n",
      "y_subset: (3088,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/CVD_cleaned.csv')\n",
    "df.head()\n",
    "\n",
    "random_state = 56\n",
    "\n",
    "X = df.drop(labels=['Heart_Disease'], axis=1)\n",
    "y = df['Heart_Disease']\n",
    "\n",
    "label_mapping = {'No': 0, 'Yes': 1}\n",
    "y = y.map(label_mapping)\n",
    "\n",
    "X_subset, _, y_subset, _ = train_test_split(X, y, test_size=0.99, stratify=y, random_state=56)\n",
    "\n",
    "print(\"Full Set\")\n",
    "print('X:',X.shape)\n",
    "print('y:',y.shape)\n",
    "#print(\"Subset Set\")\n",
    "print('X_subset:',X_subset.shape)\n",
    "print('y_subset:',y_subset.shape)\n",
    "\n",
    "cat_ftrs = ['Checkup','Exercise','Skin_Cancer','Other_Cancer', 'Depression', 'Diabetes', 'Arthritis', 'Sex','Smoking_History']\n",
    "ordinal_ftrs = ['General_Health','Age_Category',]\n",
    "ordinal_cats = [['Poor','Fair','Good','Very Good','Excellent'],\\\n",
    "               ['18-24','25-29','30-34','35-39','40-44','45-49','50-54','55-59','60-64','65-69','70-74','75-79','80+']]\n",
    "num_ftrs = ['Height_(cm)', 'Weight_(kg)', 'BMI', 'Alcohol_Consumption', 'Fruit_Consumption',\n",
    "       'Green_Vegetables_Consumption', 'FriedPotato_Consumption']\n",
    "\n",
    "# one-hot encoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(sparse_output=False,handle_unknown='ignore'))])\n",
    "\n",
    "# ordinal encoder\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('ordinal', OrdinalEncoder(categories = ordinal_cats))])\n",
    "\n",
    "# standard scaler\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# collect the transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs),\n",
    "        ('ord', ordinal_transformer, ordinal_ftrs)])\n",
    "final_scaler = StandardScaler()\n",
    "prep = Pipeline(steps=[('preprocessor', preprocessor)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, Y = df['Heart_Disease'].value_counts()\n",
    "baseline_recall = Y/(N+Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random state 1\n",
      "(61771, 18)\n",
      "Fitting 2 folds for each of 108 candidates, totalling 216 fits\n",
      "Random state 2\n",
      "(61771, 18)\n",
      "Fitting 2 folds for each of 108 candidates, totalling 216 fits\n",
      "Random state 3\n",
      "(61771, 18)\n",
      "Fitting 2 folds for each of 108 candidates, totalling 216 fits\n",
      "Random state 4\n",
      "(61771, 18)\n",
      "Fitting 2 folds for each of 108 candidates, totalling 216 fits\n",
      "Random state 5\n",
      "(61771, 18)\n",
      "Fitting 2 folds for each of 108 candidates, totalling 216 fits\n"
     ]
    }
   ],
   "source": [
    "nr_states = 5\n",
    "\n",
    "xgb_scores = []\n",
    "xgb_models = []\n",
    "xgb_test_sets = []\n",
    "\n",
    "for i in range(nr_states):\n",
    "    print('Random state', i + 1)\n",
    "    \n",
    "    X_train, X_other, y_train, y_other = train_test_split(X, y, stratify=y, train_size=0.6, random_state=56*i)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, test_size=0.5, stratify=y_other, random_state=56*i)\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    df_train = pd.DataFrame(preprocessor.fit_transform(X_train), columns=preprocessor.get_feature_names_out())\n",
    "    df_val = pd.DataFrame(preprocessor.transform(X_val), columns=preprocessor.get_feature_names_out())\n",
    "    df_test = pd.DataFrame(preprocessor.transform(X_test), columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "    XGB = xgb.XGBClassifier(early_stopping_rounds=10, n_jobs = -1)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [100,200, 500],\n",
    "        'max_depth': [1, 3, 10, 30],\n",
    "        'gamma': [0.1, 1, 10],\n",
    "        'learning_rate': [0.01],\n",
    "        'scale_pos_weight': [1, 11, 12]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(XGB, param_grid, scoring='recall', cv=2, verbose=1, n_jobs=-1)\n",
    "    grid.fit(df_train, y_train, eval_set=[(df_val, y_val)], verbose=False)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    xgb_models.append(best_model)\n",
    "\n",
    "    for X_save, y_save, dataset_name_save in [(df_train, y_train, 'train'), (df_val, y_val, 'validation'), (df_test, y_test, 'test')]:\n",
    "        y_pred = best_model.predict(X_save)\n",
    "        precision, recall, _ = precision_recall_curve(y_save, best_model.predict_proba(X_save)[:, 1])\n",
    "        auc_pr = auc(recall, precision)\n",
    "        score = {\n",
    "            'dataset': dataset_name_save,\n",
    "            'state': i + 1,\n",
    "            'precision': precision_score(y_save, y_pred),\n",
    "            'recall': recall_score(y_save, y_pred),\n",
    "            'f2': fbeta_score(y_save, y_pred, beta=2),\n",
    "            'auc-pr': auc_pr\n",
    "        }\n",
    "        xgb_scores.append(score)\n",
    "\n",
    "    xgb_test_sets.append({'X_test': df_test, 'y_test': y_test, 'state': i + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train recall mean: 0.8651271440966429\n",
      "validation recall mean: 0.8626351621946335\n",
      "test recall mean: 0.8657989587505007\n",
      "test recall standard deviation: 0.004295422563970166\n",
      "182.7407 standard deviations above the baseline\n"
     ]
    }
   ],
   "source": [
    "xgb_score = pd.DataFrame(xgb_scores)\n",
    "xgb_score\n",
    "xgb_train_recall = xgb_score[xgb_score['dataset'] == 'train']['recall']\n",
    "xgb_val_recall = xgb_score[xgb_score['dataset'] == 'validation']['recall']\n",
    "xgb_test_recall = xgb_score[xgb_score['dataset'] == 'test']['recall']\n",
    "\n",
    "print('train recall mean:',np.mean(xgb_train_recall))\n",
    "print('validation recall mean:',np.mean(xgb_val_recall))\n",
    "print('test recall mean:',np.mean(xgb_test_recall))\n",
    "print('test recall standard deviation:',np.std(xgb_test_recall))\n",
    "print(round((np.mean(xgb_test_recall)-baseline_recall)/np.std(xgb_test_recall),4),'standard deviations above the baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xgb_results_1204.pkl', 'wb') as file:\n",
    "    pickle.dump({'scores': xgb_scores, 'models': xgb_models, 'test_sets': xgb_test_sets}, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bc7e3361d9d4e6acf19e6d3debb289c8e445d5570286ce39fb3214745b4bcdf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
